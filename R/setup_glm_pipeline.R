#' Main worker function for setting up an analysis pipeline
#'
#' @param analysis_name A character string providing a useful name for identifying this analysis. Practically, this
#'   influences the top-level folder name of the group analysis outputs, as well as the name of .RData objects
#'   saved by this function to the output directory for the analysis.
#' @param scheduler Which HPC scheduler system should be used for queueing jobs. Options are 'slurm', 'torque', or 'local'.
#' @param subject_data A data.frame containing all subject-level data such as age, sex, or other covariates. Columns
#'   from \code{subject_data} can be used as covariates in group (aka 'level 3') analyses. If \code{NULL}, then
#'   this will be distilled from \code{trial_data} by looking for variables that vary at the same
#'   level as \code{vm["id"]}.
#' @param run_data A data.frame containing all run-level data such as run condition or run number. Columns from
#'   \code{run_data} can be used as covariates in subject (aka 'level 2') analyses. If \code{NULL}, this will be
#'   distilled from \code{trial_data} by looking for variables that vary at the same level as \code{vm["run_number"]}.
#' @param trial_data A data.frame containing trial-level statistics for all subjects. Data should be stacked in long
#'   form such that each row represents a single trial for a given subject and the number of total rows is subjects x trials.
#'   If you wish, you can pass a single trial-level data frame that also contains all run-level and subject-level covariates
#'   (i.e., a combined long format, where variables at different levels are all included as columns). In this case,
#'   \code{setup_glm_pipeline} will detect which variables occur at each level and parse these accordingly into
#'   \code{subject_data} and \code{run_data}.
#' @param vm A named character vector containing key identifying columns in \code{subject_data} and
#'   \code{trial_data}. Minimally, this vector should contain the elements 'id'
#' @param id_column A character string indicating the name of the subject identifier in \code{subject_data} and \code{trial_data}.
#' @param bad_ids An optional vector of ids in \code{subject_data} and \code{trial_data} that should be excluded from analysis.
#' @param mr_dir_column A character string indicating the column name in \code{subject_data} containing the folder for each
#'   subject's data. Default is "mr_dir".
#' @param fmri_file_regex A character string containing a Perl-compatible regular expression for the subfolder and filename
#'   within the \code{mr_dir} field in \code{subject_data}.
#' @param tr The repetition time of the scanning sequence in seconds. Used for setting up design matrices
#' @param output_directory The output directory for all configuration and job submission files for this analysis.
#'   Default is a subfolder called "glm_out" in the current working directory.
#' @param drop_volumes The number of volumes to drop from the fMRI data and convolved regressors prior to analysis.
#'   Default is 0.
#' @param use_preconvolve A boolean indicating whether to enter convolved regressors into the GLM estimation software
#'   (e.g., FSL FILM/FEAT). If \code{TRUE}, all regressors will be generated by build_design_matrix. 
#'   If \code{FALSE}, onset-duration-value timing will be entered and convolution will be handled internally
#'   by the GLM software. I recommend \code{TRUE} for consistency.
#' @param l1_models An \code{l1_model_set} object containing all level 1 (run) models to be included in GLM pipeline.
#'   If "prompt" is passed, \code{setup_glm_pipeline} will call \code{build_l1_models} to setup l1 models
#'   interactively. Optionally, this argument can be \code{NULL} if you want to setup l1 models later, though
#'   the resulting object will not be functional within the pipeline until l1 models are provided.
#' @param l2_models An \code{hi_model_set} object containing all level 2 (subject) models to be included in GLM pipeline.
#'   If "prompt" is passed, \code{setup_glm_pipeline} will call \code{build_l2_models} to setup l2 models
#'   interactively. Optionally, this argument can be \code{NULL} if you want to setup l2 models later, though
#'   the resulting object will not be functional within the pipeline until l2 models are provided.
#' @param l3_models An \code{hi_model_set} object containing all level 3 (sample) models to be included in GLM pipeline.
#'   If "prompt" is passed, \code{setup_glm_pipeline} will call \code{build_l3_models} to setup l3 models
#'   interactively. Optionally, this argument can be \code{NULL} if you want to setup l3 models later, though
#'   the resulting object will not be functional within the pipeline until l3 models are provided.
#' @param glm_software Which fMRI analysis package to use in the analysis. Options are "FSL", "SPM", or "AFNI"
#'   (case insensitive).
#' @param n_expected_runs Number of expected runs per subject. Used to determine 2- versus 3-level analysis
#'   (for FSL), and for providing feedback about subjects who have unexpected numbers of runs.
#' @param additional A list of additional metadata that will be added to the \code{glm.pipeline} object returned
#'   by the function. This can be useful if there are other identifiers that you want for long-term storage or
#'   off-shoot functions.
#' @param lgr_threshold The logging threshold used to determine whether to output messages of different severity to
#'   the screen and to log files. Default is "info", which produces all messages, warnings, and errors, but not debug
#'   or trace statements. To output only concerning errors, change to "error". See: 
#'   \url{https://s-fleck.github.io/lgr/articles/lgr.html} for details.
#'
#' @importFrom checkmate assert_subset assert_data_frame assert_number assert_integerish assert_list assert_logical
#'    test_string test_class
#' @importFrom dplyr mutate_at group_by select vars inner_join filter count is_grouped_df ungroup
#' @export
setup_glm_pipeline <- function(analysis_name = "glm_analysis", scheduler = "slurm",
                               output_directory = file.path(getwd(), analysis_name),
                               subject_data = NULL, run_data = NULL, trial_data = NULL,
                               group_output_directory = "default",
                               output_locations = "default",
                               vm = c(
                                 id = "id", session = "session", run_number = "run_number", trial = "trial",
                                 run_trial = "run_trial", mr_dir = "mr_dir", run_nifti = "run_nifti",
                                 exclude_subject = "exclude_subject"
                               ),
                               bad_ids = NULL, tr = NULL,
                               fmri_file_regex = ".*\\.nii(\\.gz)?", fmri_path_regex = NULL,
                               run_number_regex = ".*run-*([0-9]+).*", drop_volumes = 0L,
                               l1_models = "prompt", l2_models = "prompt", l3_models = "prompt",
                               glm_software = "fsl", n_expected_runs = 1L,
                               use_preconvolve = TRUE,
                               glm_settings = "default",
                               confound_settings = list(
                                 motion_params_file = "motion.par", # assumed to be in the same folder as the fmri run NIfTIs -- use *relative* paths to alter this assumption
                                 motion_params_colnames = c("rx", "ry", "rz", "tx", "ty", "tz"),
                                 confound_input_file = NULL, # assumed to be in the same folder as the fmri run NIfTIs -- use *relative* paths to alter this assumption
                                 confound_input_colnames = NULL, # names of confound columns -- if null, we will attempt to find a header row
                                 l1_confound_regressors = NULL, # column names in motion_params_file and/or confound_input_file
                                 exclude_run = "mean(FD) > 0.9 | max(FD) > 0.5",
                                 truncate_run = NULL, # "FD > 1 & volume > last_onset"
                                 exclude_subject = NULL,
                                 spike_volumes = "FD > 0.9"
                               ),
                               parallel = list(
                                 # number of cores used when looping over l1 setup of design matrices and syntax for each subject
                                 l1_setup_cores = 4L,

                                 # number of cores used  when looping over l1 model variants in push_pipeline
                                 pipeline_cores = "default"
                               ), additional = list(
                                 # additional feat level 1 settings (uses internal FSL nomenclature)
                                 feat_l1_args = list(z_thresh = 1.96, prob_thresh = .05)
                               ),
                               lgr_threshold = "info") {
  checkmate::assert_string(analysis_name) # must be scalar string
  checkmate::assert_subset(scheduler, c("slurm", "sbatch", "torque", "qsub", "local", "sh"), empty.ok = FALSE)
  checkmate::assert_data_frame(subject_data, null.ok = TRUE)
  checkmate::assert_data_frame(run_data, null.ok = TRUE)
  checkmate::assert_data_frame(trial_data)
  checkmate::assert_character(vm, unique = TRUE) # all values of vm must refer to distinct columns

  # would be insane to have super-long TR (suggests milliseconds, not seconds passed in)
  checkmate::assert_number(tr, lower = 0.01, upper = 20)
  checkmate::assert_string(fmri_file_regex, null.ok = TRUE)
  checkmate::assert_string(run_number_regex, null.ok = TRUE)
  checkmate::assert_integerish(drop_volumes)
  checkmate::assert_character(glm_software)
  checkmate::assert_logical(use_preconvolve, null.ok = FALSE)

  glm_software <- tolower(glm_software)
  checkmate::assert_subset(glm_software, c("fsl", "spm", "afni"))
  checkmate::assert_integerish(n_expected_runs, lower = 1L, null.ok = TRUE)
  if (checkmate::test_string(lgr_threshold)) {
    checkmate::assert_subset(lgr_threshold, c("off", "fatal", "error", "warn", "info", "debug", "trace", "all"))
  } else if (checkmate::test_number(lgr_threshold)) {
    checkmate::assert_integerish(lgr_threshold, lower = 0, len = 1L, all.missing = TRUE)
    lgr_threshold <- as.integer(lgr_threshold)
  } else {
    lgr_threshold <- "info" # default to info in case of weird input
  }

  lg <- lgr::get_logger("glm_pipeline/setup_glm_pipeline")
  lg$set_threshold(lgr_threshold)

  if (!basename(output_directory) == analysis_name) {
    lg$info("Appending analysis_name %s to output_directory %s", analysis_name, output_directory)
    output_directory <- file.path(output_directory, analysis_name)
  }

  # setup output directory, if needed
  if (!dir.exists(output_directory)) {
    lg$info("Setting up output directory for pipeline: %s", output_directory)
    dir.create(output_directory, recursive = TRUE)
  }

  # always use the full path internally
  output_directory <- normalizePath(output_directory)

  # validate and fill in variable mapping vector (if user only passes some fields)
  default_vm <- c(
    id = "id", session = "session", trial = "trial", run_trial = "run_trial",
    mr_dir = "mr_dir", run_nifti = "run_nifti", run_number = "run_number"
  )

  default_vm[names(vm)] <- vm # override defaults with user inputs
  vm <- default_vm # reassign full vm

  # having trial_data as grouped can cause problems (e.g., having the grouping variable unexpectedly come back as a column in select)
  if (is_grouped_df(trial_data)) trial_data <- trial_data %>% ungroup()

  # code default session of 1, if missing
  if (!vm["session"] %in% names(trial_data)) {
    trial_data[[vm["session"]]] <- 1
  }

  # code default run of 1, if missing
  if (!vm["run_number"] %in% names(trial_data)) {
    trial_data[[vm["run_number"]]] <- 1
  }

  # enforce id column in trial_data
  stopifnot(vm["id"] %in% names(trial_data))

  # rename columns of trial data frame to use internal nomenclature (trial_data modified in place)
  names(trial_data) <- names_to_internal(trial_data, vm)

  # whether to run a 2-level or 3-level analysis
  multi_run <- ifelse(length(unique(trial_data$run_number)) > 1L, TRUE, FALSE)

  # create run data, if needed
  if (is.null(run_data)) {
    lg$info("Distilling run_data object from trial_data by finding variables that vary at run level")

    variation_df <- trial_data %>%
      group_by(id, session, run_number) %>%
      mutate_at(vars(everything()), ~ length(unique(.))) %>%
      ungroup()

    # should include the id and run columns
    one_cols <- names(which(sapply(variation_df, function(col) {
      all(col == 1)
    }) == TRUE))

    lg$info(paste0("Retaining columns: ", paste(one_cols, collapse = ", ")))

    # at present, this will keep all subject-level covariates, too. Maybe correct later?
    run_data <- trial_data %>%
      select(!!one_cols) %>%
      group_by(id, session, run_number) %>%
      filter(row_number() == 1) %>%
      ungroup()
  } else {
    # if we are working from an external run_data object, rename variables
    # rename columns of run data frame to use internal nomenclature
    names(run_data) <- names_to_internal(run_data, vm)
  }

  # having run_data as grouped can cause problems (e.g., having the grouping variable unexpectedly come back as a column in select)
  if (is_grouped_df(run_data)) run_data <- run_data %>% ungroup()

  stopifnot("id" %in% names(run_data))
  if (!"session" %in% names(run_data)) run_data$session <- 1

  # create subject data
  if (is.null(subject_data)) {
    lg$info("Distilling subject_data object from trial_data by finding variables that vary at subject level")

    variation_df <- trial_data %>%
      group_by(id, session) %>%
      mutate_at(vars(everything()), ~ length(unique(.))) %>%
      ungroup()

    # should include the id column itself
    one_cols <- names(which(sapply(variation_df, function(col) {
      all(col == 1)
    }) == TRUE))

    message("Retaining columns: ", paste(one_cols, collapse = ", "))

    subject_data <- trial_data %>%
      select(!!one_cols) %>%
      group_by(id, session) %>%
      filter(row_number() == 1) %>%
      ungroup()
  } else {
    # if we are working from an external run_data object, rename variables
    # rename columns of run data frame to use internal nomenclature (run_data modified in place)
    names(subject_data) <- names_to_internal(subject_data, vm)
  }

  # having subject_data as grouped can cause problems (e.g., having the grouping variable unexpectedly come back as a column in select)
  if (is_grouped_df(subject_data)) subject_data <- subject_data %>% ungroup()

  # can't really get traction without this!
  stopifnot("mr_dir" %in% names(subject_data))
  stopifnot("id" %in% names(subject_data))
  if (!"session" %in% names(subject_data)) subject_data$session <- 1

  # TODO: should probably look at names in subject, run, and trial data to make sure they all line up

  # compare ids in subject_data, run_data, and trial_data
  subject_data_ids <- unique(subject_data$id)
  run_data_ids <- unique(run_data$id)
  trial_data_ids <- unique(trial_data$id)

  all_ids <- named_list(subject_data_ids, run_data_ids, trial_data_ids)
  match_ids <- Reduce(intersect, all_ids) # only keep ids present at all three levels
  union_ids <- Reduce(union, all_ids) # only keep ids present at all three levels

  # print all pairwise differences in ids
  setdiff_list_combn(all_ids)

  if (length(match_ids) == 0L) {
    msg <- "No ids are in common across subject_data, run_data, and trial_data! We cannot proceed with setup."
    lg$error(msg)
    stop(msg)
  } else if (length(match_ids) < length(union_ids)) {
    lg$warn("The ids in subject_data, run_data, and trial_data are not identical. Only the ids in common will be analyzed!")
    lg$warn(glue("Number of non-matching ids: {length(union_ids) - length(match_ids)}"))
    lg$warn(glue("Dropped ids: {paste(setdiff(union_ids, match_ids), collapse=', ')}"))
  }

  subject_data <- subject_data %>% dplyr::filter(id %in% !!match_ids)
  run_data <- run_data %>% dplyr::filter(id %in% !!match_ids)
  trial_data <- trial_data %>% dplyr::filter(id %in% !!match_ids)

  # enforce that subject id + session must be unique (only one row per combination)
  subj_counts <- subject_data %>% count(id, session)
  if (any(subj_counts$n > 1)) {
    subj_dupes <- subj_counts %>% dplyr::filter(n > 1)
    msg <- "At least one id + session combination in subject_data is duplicated. All rows in subject_data must represent unique observations!"
    lg$error(msg)
    lg$error("Problematic entries: ")
    lg$error("%s", capture.output(print(subject_data %>% dplyr::inner_join(subj_dupes, by=c("id", "session")))))
    stop(msg)
  }

  # force as.character so that class attributes always match between missing runs and valid runs that are truncated (affects rbindlist)
  # https://github.com/Rdatatable/data.table/issues/3911
  if ("mr_dir" %in% names(subject_data)) subject_data$mr_dir <- as.character(subject_data$mr_dir)
  if ("mr_dir" %in% names(run_data)) run_data$mr_dir <- as.character(run_data$mr_dir)
  if ("run_nifti" %in% names(run_data)) run_data$run_nifti <- as.character(run_data$run_nifti)
  if ("confound_input_file" %in% names(run_data)) run_data$confound_input_file <- as.character(run_data$confound_input_file)

  if (!is.null(l1_models)) {
    if (checkmate::test_string(l1_models) && l1_models[1L] == "prompt") {
      l1_models <- build_l1_models(trial_data = trial_data)
    } else if (!checkmate::test_class(l1_models, "l1_model_set")) {
      stop("l1_models argument is not of class l1_model_set. Use build_l1_model to create this.")
    }
  } # else allow nulls in case user wants to specify things later

  gpa <- list(
    # metadata
    analysis_name = analysis_name,
    scheduler = scheduler,
    output_directory = output_directory,
    subject_data = subject_data,
    run_data = run_data,
    trial_data = trial_data,
    vm = vm,
    bad_ids = bad_ids,
    tr = tr,
    multi_run = multi_run, # 2- or 3-level analysis
    glm_settings = glm_settings,
    confound_settings = confound_settings,
    n_expected_runs = n_expected_runs,
    output_locations = output_locations,

    # l1 analysis details
    fmri_file_regex = fmri_file_regex,
    fmri_path_regex = fmri_path_regex,
    run_number_regex = run_number_regex,
    drop_volumes = drop_volumes,
    use_preconvolve = use_preconvolve,
    l1_models = l1_models,

    # l2 analysis details
    l2_models = l2_models,

    # l3 analysis details
    l3_models = l3_models,
    finalize_complete = FALSE,
    lgr_threshold = lgr_threshold
  )

  # validate and populate any other pipeline details before execution
  # gpa <- finalize_pipeline_configuration(gpa)

  class(gpa) <- c("list", "glm_pipeline_arguments")

  # populate $output_locations
  gpa <- setup_output_locations(gpa, lg)

  # copy in settings passed by user
  gpa$parallel <- parallel

  # add name of node/host on which this is run (useful for logic about different compute environments)
  info <- Sys.info()
  gpa$nodename <- info["nodename"]
  gpa$sys_info <- info # populate full system information to object

  # populate $parallel
  gpa <- setup_parallel_settings(gpa, lg)

  # initial checks on compute environment
  test_compute_environment(gpa, stop_on_fail=FALSE)

  return(gpa)
}