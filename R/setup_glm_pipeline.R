#' Main worker function for setting up an analysis pipeline
#'
#' @param analysis_name A character string providing a useful name for identifying this analysis. Practically, this
#'   influences the top-level folder name of the group analysis outputs, as well as the name of .RData objects
#'   saved by this function to the working directory for the analysis.
#' @param scheduler Which HPC scheduler system should be used for queueing jobs. Options are 'slurm' or 'torque'.
#' @param subject_data A data.frame containing all subject-level data such as age, sex, or other covariates. Columns
#'   from \code{subject_data} can be used as covariates in group (aka 'level 3') analyses. If \code{NULL}, then
#'   this will be distilled from \code{trial_data} by looking for variables that vary at the same
#'   level as \code{vm["id"]}.
#' @param run_data A data.frame containing all run-level data such as run condition or run number. Columns from
#'   \code{run_data} can be used as covariates in subject (aka 'level 2') analyses. If \code{NULL}, this will be
#'   distilled from \code{trial_data} by looking for variables that vary at the same level as \code{vm["run_number"]}.
#' @param trial_data A data.frame containing trial-level statistics for all subjects. Data should be stacked in long
#'   form such that each row represents a single trial for a given subject and the number of total rows is subjects x trials.
#'   If you wish, you can pass a single trial-level data frame that also contains all run-level and subject-level covariates
#'   (i.e., a combined long format, where variables at different levels are all included as columns). In this case,
#'   \code{setup_glm_pipeline} will detect which variables occur at each level and parse these accordingly into
#'   \code{subject_data} and \code{run_data}.
#' @param vm A named character vector containing key identifying columns in \code{subject_data} and
#'   \code{trial_data}. Minimally, this vector should contain the elements 'id' 
#' @param id_column A character string indicating the name of the subject identifier in \code{subject_data} and \code{trial_data}.
#' @param bad_ids An optional vector of ids in \code{subject_data} and \code{trial_data} that should be excluded from analysis.
#' @param mr_dir_column A character string indicating the column name in \code{subject_data} containing the folder for each
#'   subject's data. Default is "mr_dir".
#' @param fmri_file_regex A character string containing a Perl-compatible regular expression for the subfolder and filename
#'   within the \code{mr_dir} field in \code{subject_data}.
#' @param tr The repetition time of the scanning sequence in seconds. Used for setting up design matrices
#' @param working_directory The working directory for all configuration and job submission files for this analysis.
#'   Default is a subfolder called "glm_out" in the current working directory.
#' @param drop_volumes The number of volumes to drop from the fMRI data and convolved regressors prior to analysis.
#'   Default is 0.
#' @param use_preconvolve A boolean indicating whether to enter convolved regressors into the GLM estimation software
#'   (e.g., FSL FILM/FEAT). If \code{TRUE}, all regressors will be generated by build_design_matrix in the dependlab
#'   R package. If \code{FALSE}, onset-duration-value timing will be entered and convolution will be handled internally
#'   by the GLM software. I recommend \code{TRUE} for consistency.
#' @param l1_models An \code{l1_model_set} object containing all level 1 (run) models to be included in GLM pipeline.
#'   If "prompt" is passed, \code{setup_glm_pipeline} will call \code{build_l1_models} to setup l1 models
#'   interactively. Optionally, this argument can be \code{NULL} if you want to setup l1 models later, though
#'   the resulting object will not be functional within the pipeline until l1 models are provided.
#' @param l2_models An \code{hi_model_set} object containing all level 2 (subject) models to be included in GLM pipeline.
#'   If "prompt" is passed, \code{setup_glm_pipeline} will call \code{build_l2_models} to setup l2 models
#'   interactively. Optionally, this argument can be \code{NULL} if you want to setup l2 models later, though
#'   the resulting object will not be functional within the pipeline until l2 models are provided.
#' @param l3_models An \code{hi_model_set} object containing all level 3 (sample) models to be included in GLM pipeline.
#'   If "prompt" is passed, \code{setup_glm_pipeline} will call \code{build_l3_models} to setup l3 models
#'   interactively. Optionally, this argument can be \code{NULL} if you want to setup l3 models later, though
#'   the resulting object will not be functional within the pipeline until l3 models are provided.
#' @param glm_software Which fMRI analysis package to use in the analysis. Options are "FSL", "SPM", or "AFNI"
#'   (case insensitive).
#' @param additional A list of additional metadata that will be added to the \code{glm.pipeline} object returned
#'   by the function. This can be useful if there are other identifiers that you want for long-term storage or
#'   off-shoot functions.
#'
#' @importFrom checkmate assert_subset assert_data_frame assert_number assert_integerish assert_list assert_logical
#'    test_string test_class
setup_glm_pipeline <- function(analysis_name="glm_analysis", scheduler="slurm", working_directory=file.path(getwd(), "glm_out"),
                               group_output_directory="default",
                               subject_data=NULL, run_data=NULL, trial_data=NULL,
                               vm=c(id="id", session="session", run_number="run_number", trial="trial", run_trial="run_trial",
                                 mr_dir="mr_dir", run_nifti="run_nifti"),
                               bad_ids=NULL, tr=NULL,
                               fmri_file_regex=".*\\.nii(\\.gz)?", fmri_path_regex=NULL, run_number_regex=".*run-*([0-9]+).*",
                               drop_volumes=0L, l1_models="prompt", l2_models="prompt", l3_models="prompt",
                               glm_software="fsl",
                               use_preconvolve=TRUE, truncate_runs=FALSE, force_l1_creation=FALSE,
                               confound_settings=list(
                                 motion_params_file="motion.par", #assumed to be in the same folder as the fmri run NIfTIs -- use *relative* paths to alter this assumption
                                 motion_params_columns=c("rx", "ry", "rz", "tx", "ty", "tz"),
                                 confound_file=NULL, #assumed to be in the same folder as the fmri run NIfTIs -- use *relative* paths to alter this assumption
                                 confound_columns=NULL, #names of confound columns -- if null, we will attempt to find a header row
                                 l1_confound_regressors=NULL, #column names in motion_params and/or confound_file
                                 exclude_run=expression(mean(FD) > 0.9 | max(FD) > 0.5),
                                 exclude_subject=expression(nruns < 4),
                                 spike_volume=expression(FD > 0.9)
                               ),
                               parallel=list(
                                 l1_setup_cores = 1L, #number of cores used when looping over l1 setup of design matrices and syntax for each subject
                                 pipeline_cores = "default" #number of cores used  when looping over l1 model variants in push_pipeline
                               ), additional=list(
                                 feat_l1_args=list(z_thresh=1.96, prob_thresh=.05) #additional feat level 1 settings (uses internal FSL nomenclature)
                               )) {
  
  checkmate::assert_string(analysis_name) #must be scalar string
  checkmate::assert_subset(scheduler, c("slurm", "sbatch", "torque", "qsub"), empty.ok=FALSE)
  checkmate::assert_data_frame(subject_data, null.ok=TRUE)
  checkmate::assert_data_frame(run_data, null.ok=TRUE)
  checkmate::assert_data_frame(trial_data)
  checkmate::assert_character(vm, unique=TRUE) #all values of vm must refer to distinct columns
  # would be insane to have super-long TR (suggests milliseconds, not seconds passed in)
  checkmate::assert_number(tr, lower=0.01, upper=20)
  checkmate::assert_string(fmri_file_regex, null.ok=TRUE)
  checkmate::assert_string(run_number_regex, null.ok=TRUE)
  checkmate::assert_integerish(drop_volumes)
  checkmate::assert_character(glm_software)
  checkmate::assert_logical(use_preconvolve, null.ok=FALSE)
  checkmate::assert_logical(truncate_runs, null.ok=FALSE)
  checkmate::assert_logical(force_l1_creation, null.ok=FALSE)

  glm_software <- tolower(glm_software)
  checkmate::assert_subset(glm_software, c("fsl", "spm", "afni"))

  #setup working directory, if needed
  if (!dir.exists(working_directory)) {
    message("Setting up working directory for pipeline: ", working_directory)
    dir.create(working_directory, recursive=TRUE)
  }

  # validate and fill in variable mapping vector (if user only passes some fields)
  default_vm <- c(
    id = "id", session = "session", trial = "trial", run_trial = "run_trial",
    mr_dir = "mr_dir", run_nifti = "run_nifti", run_number = "run_number"
  )

  default_vm[names(vm)] <- vm #override defaults with user inputs
  vm <- default_vm #reassign full vm

  # code default session of 1, if missing
  if (!vm["session"] %in% names(trial_data)) { trial_data[[ vm["session"] ]] <- 1 }

  # code default run of 1, if missing
  if (!vm["run_number"] %in% names(trial_data)) { trial_data[[ vm["run_number"] ]] <- 1 }

  # enforce id column in trial_data
  stopifnot(vm["id"] %in% names(trial_data))

  # rename columns of trial data frame to use internal nomenclature (trial_data modified in place)
  names(trial_data) <- names_to_internal(trial_data, vm)

  # whether to run a 2-level or 3-level analysis
  multi_run <- ifelse(length(unique(trial_data$run_number)) > 1L, TRUE, FALSE)

  #create run data, if needed
  if (is.null(run_data) && isTRUE(multi_run)) {
    message("Distilling run_data object from trial_data by finding variables that vary at run level")

    variation_df <- trial_data %>% group_by(id, session, run_number) %>%
      mutate_at(vars(everything()), ~length(unique(.))) %>%
      ungroup()

    #should include the id and run columns
    one_cols <- names(which(sapply(variation_df, function(col) { all(col==1) }) ==TRUE))

    message("Retaining columns: ", paste(one_cols, collapse=", "))

    # at present, this will keep all subject-level covariates, too. Maybe correct later?
    run_data <- trial_data %>%
      select(!!one_cols) %>%
      group_by(id, session, run_number) %>%
      filter(row_number() == 1) %>%
      ungroup()
  } else {
    # if we are working from an external run_data object, rename variables
    # rename columns of run data frame to use internal nomenclature
    names(run_data) <- names_to_internal(run_data, vm)
  }

  stopifnot("id" %in% names(run_data))
  if (!"session" %in% names(run_data)) run_data$session <- 1

  #create subject data
  if (is.null(subject_data)) {
    message("Distilling subject_data object from trial_data by finding variables that vary at subject level")

    variation_df <- trial_data %>% 
      group_by(id, session) %>%
      mutate_at(vars(everything()), ~length(unique(.))) %>%
      ungroup()

    #should include the id column itself
    one_cols <- names(which(sapply(variation_df, function(col) { all(col==1) }) == TRUE))

    message("Retaining columns: ", paste(one_cols, collapse=", "))

    subject_data <- trial_data %>%
      select(!!one_cols) %>%
      group_by(id, session) %>%
      filter(row_number() == 1) %>%
      ungroup()
  } else {
    # if we are working from an external run_data object, rename variables
    # rename columns of run data frame to use internal nomenclature (run_data modified in place)
    names(subject_data) <- names_to_internal(subject_data, vm)
  }

  #can't really get traction without this!
  stopifnot("mr_dir" %in% names(subject_data))
  stopifnot("id" %in% names(subject_data))
  if (!"session" %in% names(subject_data)) subject_data$session <- 1

  #TODO: should probably look at names in subject, run, and trial data to make sure they all line up

  if (!is.null(l1_models)) {
    if (checkmate::test_string(l1_models) && l1_models[1L] == "prompt") {
      l1_models <- build_l1_models(trial_data, variable_mapping=vm)
    } else if (!checkmate::test_class(l1_models, "l1_model_set")) {
      stop("l1_models argument is not of class l1_model_set. Use build_l1_model to create this.")
    }
  } # else allow nulls in case user wants to specify things later

  gpa <- list(
    #metadata
    analysis_name=analysis_name,
    scheduler=scheduler,
    working_directory=working_directory,
    subject_data = subject_data,
    run_data = run_data,
    trial_data = trial_data,
    vm = vm,
    bad_ids=bad_ids,
    tr=tr,
    multi_run=multi_run, #2- or 3-level analysis
    truncate_runs=truncate_runs,
    force_l1_creation=force_l1_creation,
    confound_settings=confound_settings,

    #l1 analysis details
    fmri_file_regex=fmri_file_regex,
    fmri_path_regex=fmri_path_regex,
    run_number_regex=run_number_regex,
    drop_volumes=drop_volumes,
    use_preconvolve=use_preconvolve,
    l1_models=l1_models,

    #l2 analysis details
    l2_models = l2_models,

    #l3 analysis details
    l3_models = l3_models

  )

  #validate and populate any other pipeline details before execution
  gpa <- finalize_pipeline_configuration(gpa)

  class(gpa) <- c("list", "glm_pipeline_arguments")
  return(gpa)
}