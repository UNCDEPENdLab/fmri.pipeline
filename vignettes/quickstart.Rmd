# Overview

**`fmri.pipeline`** is an R package for running full fMRI GLM analyses in FSL and other software. It allows users to specify the models they want to run at the trial-level (Level 1), run-level (Level 2), and subject-level (Level 3). `fmri.pipeline` sets up the timing files, design files, etc., and runs all the specified models in an automated way. Each job is scheduled with specific dependencies—for example, the Level 2 analysis will only begin once the Level 1 analysis successfully completes.

`fmri.pipeline` allows users to interactively build their Level 1–3 models, making it easy to set up contrasts and models of interest. It is ideal for users with a clear conceptual understanding of what they want to do but who may not know how to execute these ideas in FSL (e.g., Feat).

A major asset of using `fmri.pipeline` is that it handles model estimation for **any combination of models across levels**. The pipeline fits all combinations of Level 1, Level 2, and Level 3 models for a given dataset.

**Example:**

Consider the following scenario:
* **Level 1**: One model includes a parametric modulator for working memory load, another includes only onsets for encoding and retrieval.
* **Level 2**: One model includes average task difficulty as a continuous covariate, another treats it as a categorical variable.
* **Level 3**: One model includes nonverbal intelligence, another includes both age and nonverbal intelligence.

In this case, `fmri.pipeline` will fit eight model combinations (2 × 2 × 2) and provide labeled outputs for each.

`fmri.pipeline` adopts the following three-level structure for fMRI analysis:

* **Level 1**: Run-level analysis of the BOLD time series. Design matrix includes convolved regressors for task-related events.
* **Level 2**: Subject-level analysis that integrates Level 1 effects across all runs. Can include between-run manipulations or covariates.
* **Level 3**: Sample-level analysis estimating effects acr qoss all subjects. Design matrix includes between-subjects predictors (e.g., age, trait anxiety).

If there is only **one run of a task per subject**, only Level 1 and Level 3 models are built and fit. There is no Level 2 analysis. While some might consider this the second level, `fmri.pipeline` retains its fixed nomenclature:
* Level 1 = Runs
* Level 2 = Within-subject run combination
* Level 3 = Across-subject analysis

---

# Setting Up Your Data

`fmri.pipeline` expects three dataframes:
1. Trial-level
2. Run-level
3. Subject-level

## Required Columns

* `id: A character string or number that uniquely identifies a single subject in the dataset. 
* `trial`: The sequential order in which tasks were presented in a run, spanning across blocks. For example, if you have a run of fMRI data where participants play 4 runs of a block and each block has 15 trials, the trial number may originally be listed as 1-15 for each block. Before giving your trial-level data to fmri.pipeline, the trial column should be changed to 1-60, disregarding the trial as it pertains to block. The trial number should always be in reference to the run of fMRI data. 
* `run_number`: An integer referring to the fMRI acquisition number of a task run for a single subject. For example, if a participant played a task twice in two separate fMRI acquisitions, the run_number would be 1 for the first task and 2 for the second task. If they played a task twice within one single fMRI acquisition, the run_number would be 1 for both. This is only relevant if you have multiple fMRI runs of a task. If you do not provide run_number in your input data, fmri.pipeline will add a run_number value of 1 to all subjects.
* `session`: A character string or number or number that refers to the occasion on which the data were acquired. This is only relevant to datasets where you scan subjects in multiple sessions such as a longitudinal study. If you do not provide session in your input data, fmri.pipeline will add a session value of 1 to all subjects. 
* `run_nifti`: The pathway to the preprocessed nifti file for the run to be included in analysis.

## Trial-Level Dataframe

The trial-level dataframe should include trial-varying factors such as trial onset times, durations, and any parametric modulators that should be convolved with the HRF.

A stacked long-form data frame where each row is one trial per subject for all runs. 
*Other required columns*:
* `onset`, `duration`: Onset times and duration for each event you will include in the analysis.
* `condition`, `rt`: Trial-level variables
* Any parametric modulators that should be convolved with the HRF (Note that all parametric modulators will automatically be mean-centered by run)

Each row must be one trial of data, and multiple events can be included in a single trial. For example, you can include onset and duration columns for a choice event and a feedback event in the same trial row. When you build your level 1 models, you will be able to specify different events with this information. Any variables in this data frame can be introduced as onset, duration, or value components of a given HRF-convolved regressor.

*Optional columns*:
* Interstimulus intervals
* Intertrial intervals
* Parametric modulators (automatically mean-centered by run)
* Within-run condition factors (for example, if some trials participants receive positive feedback and others they receive negative feedback, you may wish to include a contrast that just looks at brain activity when someone receives a certain type of feedback)

**TODO**
FOR EACH trial-, run-, and subject-level dataframe: 
I would include an example of these dataframes (just like 5 lines) so people get an idea of what each dataframe might end up looking like, with both the required and optional columns! 

## Run-Level Dataframe

The run-level dataframe should include any run-varying factors and the location of the nifti files you wish to include in analysis. Each row of data will be an fMRI acquisition for a specific task. 

*Required columns*:
* `id`: A character string or number that uniquely identifies a single subject in the dataset. 
* `run_nifti`: To inform the pipeline the location of the nifti file for a run
* `run_number`: An integer referring to the fMRI acquisition number of a task run for a single subject. Add this column, if multiple runs exists.
* `session`: A character string or number or number that refers to the occasion on which the data were acquired. Add this column, if multiple sessions.

Similar to the trial-level dataframe, you can include any within-subject factors you want to include in analysis (for example, if there is a run where participants receive predominantly happy faces and another run where participants receive predominantly negative feedback in your task design, you may wish to include them as contrasts in your Level 2 analysis). 

*Optional*:
* `mr_dir`: Path to the directory contianing fMRI data for each run.
* `tr`: temporal resolution of the run. This is only necessary if the temporal resolution changed during the study and certain fMRI acquisitions have differing temporal resolution. This is not needed if the temporal resolution is the same for all runs of data to be included in analysis, as the TR will be specified when you set up the analysis (see gpa object setup section). 
* Within-subject condition factors for contrasts in Level 2.
* `confound_input_file`: Path to nuisance regressors
* Optional: Run-level covariates (e.g., `emotion`, `stim_type`)


## Subject-Level Dataframe

The subject-level dataframe should include any subject-varying factors you wish to include in analysis. Each row of data will be a specific subject’s information. 

*Required columns*:
* `id`: A character string or number that uniquely identifies a single subject in the dataset. 
* `session`: A character string or number or number that refers to the occasion on which the data were acquired. Add this column, if multiple sessions.
* Subject-varying factors: group, age, sex, scores, etc., for Level 3 analysis.
* `mr_dir`: Path to the directory contianing fMRI data for each subject.

If you want to use the pipeline in manual mode, you need to provide these three dataframes as input and the pipeline can walk you through the rest of the process of building the models.

---

# Guide to Creating a `gpa` Object in `fmri.pipeline`

This guide describes how to create a `gpa` object for your dataset using the `fmri.pipeline` R package. The `gpa` object (short for **GLM Pipeline Arguments**) stores configuration for single- and multi-level GLM analysis including subject, run, and trial data, model specifications, and compute environment settings.

## Prerequisites
* Trial level data (could be loaded from a csv file)
* Run level data (could be loaded from a csv file)
* Subject level data (could be loaded from a csv file)
* L1 model specification yaml file (e.g., `my_spec.yaml`) describing the first-level model structure.

## Step 1: Prepare the Data

Use the helper functions from `fmri.pipeline` to generate the CSVs, or load and preprocess them manually.

```{r, eval=FALSE}
trial_df <- read.csv("sample_trial_data.csv.gz")
run_df <- read.csv("sample_run_data.csv")
subj_df <- read.csv("sample_subject_data.csv")
```

## Step 2: Use `setup_glm_pipeline()` to setup GLM models for each level of analysis

`setup_glm_pipeline()` is the main function to create a `gpa` object. It initializes the pipeline with the necessary parameters and data. 
If you would like to build the models in manual mode, this function requires the trial, run, and subject dataframes as inputs.

For fmri.pipeline to setup and run GLM models, the user has to specify which models to fit at each level of analysis. The first level mostly might consist of convolved regressors for task events, parametric modulators. Level 2 could consist of covariates that capture variation beytween runs. Level 3 could consist of sample/group level analysis.
You could pass `l1_model_set` object containing all level 1 (run) models to be included in GLM pipeline. The object needs to be in a specific format acceptable by the pipeline, so we do not recommend doing this.
You can also pass `"prompt"` for any of the L1, L2 or L3 model inputs, which will call the interactive model-building functions `build_l1_models()`, `build_l2_models()`, and `build_l3_models()` respectively. This allows you to specify the models interactively at this step itself.
Alternatively, you can pass `NULL` for the `l1_models`, `l2_models`, and `l3_models` arguments, which will allow you to create a `gpa` object, though this object will not be functional within the pipeline until l1 models are provided. You can build the models later by calling the functions `build_l1_models()`, `build_l2_models()`, and `build_l3_models()`. When calling these functions later, you can either specify the `from_spec_file` argument to read the model specifications from a YAML file, or you can build the models interactively by not specifying the `from_spec_file` argument.

```{r, eval=FALSE}
# first example
gpa <- setup_glm_pipeline(
  analysis_name = "my_manual_analysis",
  scheduler = "slurm",
  output_directory = "/your/output/path",
  trial_data = trial_df,
  run_data = run_df,
  subject_data = subj_df,
  
  tr = 1.0, 
  drop_volumes = 2,
  l1_models = NULL,
  l2_models = NULL,
  l3_models = NULL,
  confound_settings = list(
    l1_confound_regressors = c("csf", "dcsf", "wm", "dwm"),
    exclude_run = "max(framewise_displacement) > 5 | sum(framewise_displacement > .9)/length(framewise_displacement) > .10",
    exclude_subject = "n_good_runs < 4",
    truncate_run = "(framewise_displacement > 0.9 & time > last_offset) | (time > last_offset + last_isi)"
  ),
  parallel = list(
    finalize_time = "10:00:00",
    fsl = list(l2_feat_time = "4:00:00", l3_feat_time = "50:00:00")
  )
)

# second example
gpa <- setup_glm_pipeline(analysis_name="12may2025", 
                          scheduler="slurm",
                          output_directory = "/proj/mnhallqlab/no_backup/nmap/analysis_output/pit",
                          trial_data=trial_df, subject_data = subject_df, run_data=run_df,
                          tr=.9, drop_volumes = 2,
                          l1_models=NULL, l2_models=NULL, l3_models=NULL,
                          n_expected_runs=1,
                          confound_settings=list(
                            confound_input_colnames = c("csf", "csf_derivative1", "white_matter", "white_matter_derivative1"), # assumption
                            l1_confound_regressors = c("csf", "csf_derivative1", "white_matter", "white_matter_derivative1"),
                            na_strings=c("NA", "n/a"), #weird fmriprep-ism for confound files
                            exclude_run = "max(framewise_displacement) > 5 | sum(framewise_displacement > .5)/length(framewise_displacement) > .15", #this must evaluate to a scalar per run
                            exclude_subject = "n_good_runs < 2",
                            truncate_run = "framewise_displacement > 2 & (time > last_offset + )" # 2 seconds after last offset
                            truncate_run = "(framewise_displacement > 0.9) & (time > last_offset) | (time > last_offset + last_isi)"
                            spike_volumes = "framewise_displacement > 0.9"
                          ),
                          parallel=list(
                            fsl=list(l1_feat_alljobs_time="72:00:00"),
                            finalize_time <- "10:00:00"
                          ), lgr_threshold="debug"
)
```

**Common Input Parameters**

* `analysis_name`: a character string that uniquely names your analysis, and is used in output folder names and saved `.RData` or `.rds` files; for example, `"emotion_analysis_march2025"`.
* `scheduler`: a character string that indicates which job scheduling system to use; typically `"slurm"` on HPC clusters, or `"local"` for local analysis. Possible options are `"slurm"`, `"torque"`, `"local"`. 
* `output_directory`: a character string that specifies the folder where the pipeline output (e.g., FSF scripts, design matrices, submission scripts) will be stored. If not specified, defaults to the current working directory (`getwd()`).
* `trial_data`: a data frame containing trial-level data with one row per trial per subject. This is required and must include identifying variables (e.g., `id`, `trial`, `run_number`) and timing (e.g., `onset`, `duration` in seconds with respect to the start of the scan).
* `run_data`: a data frame containing run-level data. This is required if not embedded in `trial_data`. It should include identifying variables (e.g., `id`, `run_number`, `mr_dir`, `run_nifti`) and can also include run-level covariates (e.g., `emotion`).
* `subject_data`: a data frame containing subject-level covariates. This is required for Level 3 analyses and should include identifying variables (e.g., `id`) and demographic variables (e.g., `age`, `sex`).
* `block_data`: an optional data frame containing block-level task information (e.g., task periods spanning multiple trials). Blocks are superordinate to trials and subordinate to runs. Useful for block designs or hybrid paradigms. Data should be stacked in long form.

**Optional Inputs**

* `vm`: a named character vector specifying the key identifying variables (e.g., `id`, `run_number`, `session`, `run_number`, `block_number`, `trial`, `run_trial`, `subtrial`, `mr_dir`, `run_nifti`,`exclude_subject`). This is rarely changed and only modified if your column names differ from the defaults.
* `bad_ids`: a character vector of IDs to exclude from all levels of analysis. This is optional and can be used to filter out specific subjects.
* `mr_dir_column`: a character string specifying the column name in `run_data` that contains the directory path to the fMRI data for each run. This is optional and defaults to `"mr_dir"`.
* `fmri_file_regex`: a character string specifying the regular expression to match fMRI file names in `run_data`. This is optional and defaults to `"nifti"`.
* `run_number_regex`: a character string specifying the regular expression to match run numbers in `run_data`. This is optional and defaults to `"run_number"`.
* `tr`: a numeric value for the repetition time of the scanning sequence in seconds. Used for setting up design matrices. If this is NULL, the function will look for a `tr` field in the `run_data` object, which specifies TR at run level.
* `drop_volumes`:  an integer number of volumes to drop from the fMRI data and convolved regressors prior to analysis. Default setting is 0.
* `l1_models`: An `l1_model_set` object containing all level 1 (run) models to be included in GLM pipeline. If `prompt` is passed, `setup_glm_pipeline` will call `build_l1_models` to setup l1 models interactively. Optionally, this argument can be `NULL` if you want to setup l1 models later, though the resulting object will not be functional within the pipeline until l1 models are provided.
* `l2_models`:An `hi_model_set` object containing all level 2 (subject) models to be included in GLM pipeline.
* `l3_models`:An `hi_model_set` object containing all level 3 (sample) models to be included in GLM pipeline.
* `confound_settings`: A list containing settings for confound regressors, such as:
  * `motion_params_file`: Path to motion parameters file (e.g., `"motion.par"`).
  * `confound_input_file`: Path to nuisance regressors file (e.g., `"nuisance_regressors.txt"`).
  * `l1_confound_regressors`: List of columns to use as nuisance regressors.
  * `exclude_run`, `exclude_subject`, `truncate_run`: Expressions used to exclude runs or subjects based on motion. If you have exclude_run and/or exclude_subject as columns in your run_data or subject_data, the pipeline will use that in complement with these expressions. truncate_run can be possible to use data for a particiapnt for the timepoints when the framewise displacement was not high enough to be excluded. When an expression is provided, which returns a vector of TRUE or FALSE for each volume, then it takes one timepoint before the first TRUE value in a series and starts truncating the run from that timepoint onwards. You can also specify an expression which returns a volume number to be excluded. In addition to any columns in your confound file and `framewise_displacement` calculated by the pipeline, you can also use `time`, `volume`, `last_onset`, `last_offset` (time of last event offset in seconds) in your expressions.
* `parallel`: A list containing parallelization settings, such as SLURM job parameters like `finalize_time`, `fsl$l2_feat_time`, etc.
* `glm_settings`: A list of additional settings for GLM software, such as FSL or AFNI. This is typically left at default.
* `additional`: A list of additional settings for GLM software, such as FSL or AFNI. This is typically left at default.
* `output_locations`: A list of output directories for different levels of analysis, such as `feat_l1_directory`, `feat_l2_directory`, `feat_l3_directory`, and `sqlite_db`. These are used to specify where the results will be stored.
* `group_output_directory`: A character string specifying the directory where group-level results will be stored. This is optional and defaults to the `output_directory`.
* `lgr_threshold`: A character string specifying the logging level for the pipeline. Options include `"info"`, `"warn"`, `"error"`, `"debug"`, etc. Default is `"info"`.

## Step 3: build each level of models
**Building the models can be done in two ways: using a YAML specification file or interactively. We will discuss both methods here.**

### Setting up the L1, L2, and L3 models interactively
If you run the `build_l1_models()`, `build_l2_models()` and `build_l3_models()` functions without specifying `from_spec_file` input, you will get to build the models interactively.

```{r, eval=FALSE}
gpa <- build_l1_models(gpa)
gpa <- build_l2_models(gpa)
gpa <- build_l3_models(gpa)
```

#### Building level 1 models interactively
One way to build Level 1 models interactively is the specify `l1_models = "prompt"` in the `setup_glm_pipeline()` function. This will call the `build_l1_models()` function, which allows you to build Level 1 models interactively.
Another way is to call the `build_l1_models()` function directly after creating the `gpa` object using `setup_glm_pipeline()`.

```{r, eval=FALSE}
gpa <- build_l1_models(gpa)
```
The `build_l1_models()` function will walk you through the process of building the model you want to run at the run level.

When defining a new level 1 model you will be prompted to enter the following:

# TODO Nidhi: Maybe add 1) to 6) points within a section for event setup menu.

1) Do you want to add these columns to possible onsets?
This prompt will help you select event onset columns. It will show you a set of event columns that have the word 'onset' in their name. You can select columns from the trial-level data frame that will be used as event onsets in the model. These columns must contain onset times in seconds relative to the start of the scan. You can select multiple columns, and the pipeline will create separate regressors for each selected column. Note that event onsets must be in seconds relative to the scan start time.
If you select "No", the pipeline will ask if you would you like to modify the event onset columns? You can (1) add new onset columns, (2) delect existing onset columns, or (3) complete the onset selection.
If you choose to add new onset columns, it will present you with a list of all columns in the trial-level data frame to select as the onset columns.
Note that it is acceptable that there are addional columns selected in these steps which are eventually unused in the model.

2) Would you like to modify the event duration columns?
This prompt will help you select event duration columns. You can select columns from the trial-level data frame that will be used as event durations in the model. These columns must contain durations in seconds relative to the start of the scan. If you select "No", the pipeline will ask if you would you like to modify the event duration columns? You can (1) add new duration columns, (2) delect existing duration columns, or (3) complete the duration selection. Note that event durations must be in seconds relative to the scan start time.
If you choose to add new duration columns, it will present you with a list of all columns in the trial-level data frame to select as the duration columns.  

3) Do you want to add these columns to possible ISI/ITIs? 
This prompt will help you select interstimulus/intertrial interval columns. You can select columns from the trial-level data frame that will be used as ISI/ITI durations in the model. These columns must contain ISI/ITI durations in seconds. If you select "No", the pipeline will ask if you would you like to modify the ISI/ITI columns? You can (1) add new ISI/ITI columns, (2) delect existing ISI/ITI columns, or (3) complete the ISI/ITI selection.
If you say "Yes" to the automatically detected columns, it will ask if you would like to modify the event ISI/ITI columns. If you choose to add new ISI/ITI columns, it will present you with a list of all columns in the trial-level data frame to select as the ISI/ITI columns. ISI can be used in truncate_runs.

4) Would you like to modify the event parametric value columns?
This prompt will help you select continuous parametric modulator event values columns. You can select columns from the trial-level data frame that will be used as parametric modulators in the model. These columns must contain continuous values that will be convolved with the HRF. If you select "No", the pipeline will ask if you would like to modify the event parametric value columns? You can (1) add new parametric value columns, (2) delect existing parametric value columns, or (3) complete the parametric value selection.
If you choose to add new parametric value columns, it will present you with a list of all columns in the trial-level data frame to select as the parametric value columns.    

5) Would you like to modify the event within-subject factor columns? 
This prompt will help you select within-subject factor columns that can be used in specifying signals.. You can select columns from the trial-level data frame that will be used as within-subject factors in the model. These columns can be used to specify signals that are convolved with the HRF. If you select "No", the pipeline will ask if you would like to modify the event within-subject factor columns? You can (1) add new within-subject factor columns, (2) delect existing within-subject factor columns, or (3) complete the within-subject factor selection.
If you choose to add new within-subject factor columns, it will present you with a list of all columns in the trial-level data frame to select as the within-subject factor columns.

6) Specify all events that can be added to a GLM model.
This builds 'events' which consist of onset times, durations, and optional ITI/ISI. This prompt will prsent options to (1) add new events, (2) delect existing events, or (3) complete the event selection. If you press 1 to add an event, firstly it will ask you the event name and then provide options from the preselected event onset columns. Then it will ask you to choose the event duration. You can select a fixed duration or select a column from the previously choosen duration columns. Lastly, it will ask you to select the ITI/ISI column or specific a fixed ITI/ISI value. If you do not want to add an ITI/ISI column, you can select "No". If you select a column, it will use that column as the ITI/ISI for the event.

7) Signal setup menu to build a set of signals that can be included as regressors in the level 1 model.
This builds 'signals', which consist of an event, an event value (amplitude), and convolution and regressor settings. 
  (a) This prompt will present options to (1) add signal, (2) modify existing signal, (3) delect existing signals, or (3) complete the signal setup. If you press 1 to add a signal, it will ask you to specify the signal name. It will provide a list of previously defined events and ask you with which event is this signal aligned. Once you select an event, it will ask you if the signal should be modelled only for specific trials.
  (b) Next it will ask you what should be the pre-convolution value of the regressor. You will have the option to select (1) Unit height (1.0), (2) specify fixed value, or (3) select a parametric modulator. Note if signals are not speecified, it will default to unit height regressors with no HRF normalization. 
  (c) If you specify a parametric modulator, it will ask you if this signal is modulated by one or more within-subject factors. If yes, then you will need to specify which within-subject factor(s) modulate this signal and is there an interaction between the factors. 
  # (TODO Nidhi: there are more steps after this of setting up the contrasts, try running an example and make notes)
  (d) It will also ask how should the HRF be normalized in convolution? (1) none, (2) evtmax_1 (aka dmUBLOCK(1); HRF max of 1.0 for each event, regardless of duration), or (3) durmax_1 (aka dmUBLOCK; HRF maxing at 1.0 as events become longer (1.0 around 15 sec)). This questions is regarding what you want to do with the height of the convolved regressor as events get longer. If no normalization is performed (no rescalling of HRF), then you are saying that events which last longer have larger BOLD activity, which is not supported in the literature especially for higher order cognition. evtmax_1 option caps the HRF height at 1.0, therefore if we have longer event durations the HRF will stay at 1.0 for a longer duration. durmax_1 option allows HRF height variation attributed to duration, but it is capped at 1.0 such that the maximum height is 1.0 for longer events. We recommend for unit height regression choose `none`, for parametric signal choose `evtmax_1`.
  (e) Next it will ask you if you want to change any of the advanced options.
    (i) No temporal derivative: temporal gradient is on the gradient between assumed vs no-assumed HRF shape GLM analysis. In the assumed shape just convolves with the HRF. In the no-assumed shape we have flexible basis function which try to fit exact response within the trial. Temporal gradient is one step away from the assumed shape towards the no-assumed shape view. It takes the first time difference of the covolved regressor and adds both in the model and it gives us a better fit to the data when the actual bold response deviates from the canonical HRF. Default: No.
    (ii) Demean convolved signal: this makes sure that the intercept in the model would represent the mean of the other regressors. Default: No.
    (iii) No beta series: should the pipeline generate beta series for this signal (one regressor per trial). If you say Yes, instead of having one regressor for this event you'll have n trials regressors. Mostly this is used to look at functional connectivity as it captures the within-person variation in neural activity. Default: No. 
    (iv) No time series multiplier (PPI)

8) Model setup menu to build set of models
  (a) This prompt will prsent options to (1) add model, (2) modify model, (3) delete model, or (4) done with L1 model setup. 
  (b) If you press 1 to add an event, firstly it will ask you the model name and then provide a list of predefined signals that could be added to the model. You can add from the signals list by add the corresponding numbers separated by a space.
  (c) Next it will ask you if you want to include diagonal contrasts for each regressor. Note: If you have a parametric modulator in the model make sure to have its occurrence regressor in the model as well. 
  (d) Next contrast setup menu will appear. You will have the option to (1) add contrast, (2) show contrast, (3) delete contrast, or (4) done with contrast setup.
  # Hannah: can you add more notes on contrast setup menu.
  (e) You can keep on adding models by choosing `(1) Add model`. After you are done with model setup, you can choose `(4) Done with l1 model setup`


#### Building level 2 models interactively
`build_l2_models()` function walks you through the process of building the model you want to run at the subject level.
```{r, eval=FALSE}
gpa <- build_l2_models(gpa)
```

When defining a new level 2 model you will be prompted to enter the following:

1) Model name
A model name which will be used in folder names and output.

2) You can specify a model formula or walk through the model builder?
If you choose to specify a model formula, you can enter the formula directly. R regression model syntax is used to specify the model formula. For example: `~ emotion * wmload + run_number`. Implicitely, the model always adds the intercept term (1+) to the model if other regressors are mentioned in the specify model option.
If you choose to walk through the model builder, it will prompt you to specify the model formula interactively.

3) Choose the reference level for the each factor used in the model formula.
After adding the model formula, it will ask you to choose the reference level for dummy coding factors in the model. This affects how contrasts are coded by emmeans and it controls how you interpret the Intercept maps at levels 2 and 3, if you look at these. In general, this choice should not make a big difference, but we default to the factor level that is most frequent in the dataset.

4) Do you want to include diagonal contrasts for each regressor?
Diagonal contrasts give one coefficient for the explanatory variable, i.e., one per column of the design matrix. In FSL terms, this corresponds to the contrast of parameter estimates (COPE) parameter. Generally, you would answer "yes" to this question.

5) How should contrasts be computed by emmeans?
This is regarding how model-predicted means or contrasts should handle unbalanced designs. In many fMRI tasks, the number of trials or runs per condition is unbalanced. For example, a task might have 4 runs of scrambled faces, 2 runs of fearful faces, and 2 runs of happy faces. In this case, if you choose "Equal", the contrasts will treat all levels of a factor equally, regardless of how many observations there are for each level. If you choose "Cell", the contrasts will weight the levels according to their sample sizes.

6) Do you want to include model-predicted means for each level of the regressor?
# Hannah could you add notes here


7) Do you want to include pairwise differences for each level of the regressor?
# Hannah could you add notes here


8) Do you want to include the overall average response?
The could be duplicate contrasts in your matrix. This can occur for many benign reasons, but wyou need to specify what you want to call these contrasts in the output so that it is clear.

#### Building level 3 models interactively
`build_l3_models()` function walks you through the process of building the model you want to run at the sample level.

```{r, eval=FALSE}
gpa <- build_l3_models(gpa)
```

You can specify multiple Level 3 models — for example, one with only an intercept, and one with covariates such as age, gender, or interactions between them. This allows you to test whether effects of interest are robust across different model specifications (e.g., with or without certain covariates).
Running `build_l3_models()` launches an interactive process where you can iteratively define and edit models. It will continue prompting you to: add new models, modify existing models, delete models, until you are satisfied with your Level 3 model set.

When defining a new model you will be prompted to enter the following:
1) Model name
A model name which will be used in folder names and output.

2) Do you want to specific the model formula or walk through the model builder?
- You can specify a model formula. You need to supply the right hand side of a regression model. Syntax is similar to `lm()` function in R formula syntax.
e.g. ~ 1 (intercept only), ~ age_at_scan, ~ age_at_scan + gender, ~ age_at_scan * gender. Use ~1 for intercept only model. Implicitely, the model always adds the intercept term (1+) to the model if other regressors are mentioned in the specify model option.

3) Include diagonal contrasts for each regressor?
Diagnoal contrasts gives one coefficient for the explanatory variable i.e. one per coulumn of the design matrix. In FSL terms this corresponds to the contrast of parameter estimates (COPX) parameter. Generally say yes.

4) How should contrasts be computed by emmeans?
This is about how model-predicted means or contrasts should handle unbalanced designs.
In many fMRI tasks, the number of trials or runs per condition is unbalanced. For example a task might have 4 runs of scrambled faces, 2 runs of fearful faces, 2 runs of happy faces. In this case, if you choose "Equal", the contrasts will treat all levels of a factor equally, regardless of how many observations there are for each level. If you choose "Cell", the contrasts will weight the levels according to their sample sizes.

If you have a factor variable, it detects that it is categorical and asks you the following questions:
5) Do you want model-predicted means for each level of the factor?
6) Do you want pairwise differences between levels of the factor?

7) Include overall average response?
Generates a map for the average subject (marginalized over covariates). Generally you would answer yes. 

8) For categorical variables it will ask you:
Do you want to include model-predicted simple slopes of covariates across levels of the factor? For example, predicted age effect for a particular gender regardless if you have an interaction in the model.

9) Based on these input the pipeline will generate some contrasts for you to review. A contrast editor menu will appear. If you choose "show contrast", you will see the automatically generated contrasts. If you do not want to modify any contrasts setup, you can click "Done with contrast setup".

Back in the main build_l3_models() menu, you will see a list of all models you've defined so far and you would have the option to add another model, modify an existing model, delete a model and finish with building level 3 models.
If you would like to see the model saved in the gpa object, you can call `str(gpa$l3_models)` to see the list of models you have defined so far.

# Hannah, could you add notes on outlier deweighting option in level 3


### Setting up the L1, L2, and L3 models using a YAML specification file
You can also specify the models using a YAML specification file and passing it to the `from_spec_file` input parameter to the `build_l1_models`, `build_l2_models` and `build_l3_models` functions. The YAML file can contain the model specifications for Level 1, Level 2, and Level 3 models.
```{r, eval=FALSE}
gpa <- build_l1_models(gpa, from_spec_file = "my_spec.yaml")
gpa <- build_l2_models(gpa, from_spec_file = "my_spec.yaml")
gpa <- build_l3_models(gpa, from_spec_file = "my_spec.yaml")
```

1. First set of outer layer options are the different columns that can be used in the model specification. You will need to add which columns in your dataframe pertains to the concept of event onsets, event durations, ISI/ITI, values (parametric modulator event), and within subject factors.
The first set of outer later options can be structured as follows. The hyphen creates an unorder list:
```yaml
onsets: # event onset columns
  - clock_onset
  - feedback_onset
  
durations: # event duration columns
  - rt_csv

isis: # ISI/ITI columns
  - iti_ideal

values: # continuous parametric modulator event columns
  - v_entropy
  - pe_max

wi_factors: # within subject factors
  - emotion
```

2. The second set of outer layer options are the events that can be used in the model specification. Each event will have a name, an onset column, a duration column, and an ISI/ITI column (optional). The duration column can be a fixed value or a column from the `durations` list. The ISI/ITI column can be a fixed value or a column from the `isis` list. The events can be structured as follows:
The events can be structured as follows:
```yaml
events:
  clock:
    onset: clock_onset
    duration: rt_csv

  feedback:
    onset: feedback_onset
    duration: 0.9
    isi: iti_ideal
```

3. The third set of outer layer options are the signals that can be used in the model specification. Each signal will have a name for the signal, event with which the signal is alligned, which HRF normalization do you want to apply, how to handle the value, and .
For setting up pre-convolution value of the regressor `value_fixed` can be used. When it is set to a constant value of 1.0 will create a unit height regressor for the signal. If `value_fixed` is not specified, it defaults to 1.0. If you want to use a parametric modulator, you can specify the `parametric_modulator` field with the name of the column from the `values` list. If you specify a parametric modulator, you can also specify if this signal is modulated by one or more within-subject factors using the `wi_factors` field. These can be specified as a model formula in the format used in `lm` package eg. `wi_formula: ~ outcome - 1`. You can also specify a `trial_subset_expression` to filter trials for this signal. 
Next you can specify how the HRF convolution be normalized by using the `normalization` field with the option `none`, `evtmax_1`, or `durmax_1`. Please refer notes above for more details on these options.
Note that events and signals can have the same name. Events are the start and end timiings of events happening during the task and signals represent convolved regressors that relate to those events.
The signals can be structured as follows:
```yaml
signals:
  clock:
    event: clock
    normalization: none
    value_fixed: 1

  feedback:
    event: feedback
    normalization: none
    value_fixed: 1

  entropy_clock:
    event: clock
    trial_subset_expression: rt_csv < 4
    normalization: evtmax_1
    parametric_modulator: v_entropy
    wi_factors: emotion
    wi_formula: ~ outcome - 1

  pe_feedback:
    event: feedback
    trial_subset_expression: rt_csv < 4
    normalization: evtmax_1
    parametric_modulator: pe_max
```

4. The fourth set of outer layer options are the l1 models that can be used in the model specification. Each model will have a name, a list of signals, and contrasts settings.
Each model can include a set of signals predefined in the signals section of the yaml file and added with dashes on new lines. You can also specify whether diagonal contrasts should be included. The `contrasts` field can include a boolean value for `include_diagonal`. You can specify include_diagonal as `yes` or `no` to indicate whether diagonal contrasts should be included.
# Hannah have you done any more complicated contrasts setup that you could add information on here?
The l1 models can be structured as follows:
```yaml
l1_models:
  entropy:
    signals:
      - clock
      - feedback
      - entropy_clock
    diagonal: yes
    cell_means: no
    overall_response: no
    weights: cells
  pe:
    signals:
      - clock
      - feedback
      - pe_feedback
    contrasts:
      include_diagonal: yes
```

5. The fifth set of outer layer options are the l2 models that can be used in the model specification.
The l2 models can be structured as follows:
```yaml
l2_models:
  l2emotion:
    level: 2
    model_formula: ~emotion
    reference_level:
      emotion: 'scram'
    diagonal: yes
    cell_means: yes
    cond_means: emotion
    pairwise_diffs: emotion
    overall_response: yes
    weights: cells
    delete: EV_(Intercept) 
```

6. The sixth set of outer layer options are the l3 models that can be used in the model specification.
The l3 models can be structured as follows:
```yaml
l3_models:
  l3age:
    level: 3
    model_formula: ~age
    covariate_transform:
        age: mean
    diagonal: yes
    cell_means: no
    overall_response: yes
    weights: cells
    delete: EV_(Intercept)
    fsl_outlier_deweighting: no
```



## Step 4: Run the Pipeline
To run the pipeline, you can use the `run_glm_pipeline()` function. This function will execute all specified models across levels, scheduling jobs as needed.
```{r, eval=FALSE}
run_glm_pipeline(gpa)
``` 
# This function will:
1) Validate the `gpa` object.
2) Create output directories based on the `gpa$output_locations` settings.
3) Generate job submission scripts for each model.
4) Submit jobs to the specified scheduler (e.g., SLURM, Torque).

Note: If you are running the pipeline on a local machine, you can set `scheduler = "local"` in the `gpa` object.

When you run `run_glm_pipeline(gpa)`, it will prompt you to choose the level 1 models to run. You can select one or more models from the list of defined Level 1 models. Similarly, you can choose Level 2 and Level 3 models to run.
This steps will create separate jobs for each conceptually separate steps: finalize, setup_l1, run_l1, setup_l2, run_l2, setup_l3, run_l3. Each job will be submitted to the scheduler with the appropriate job dependencies.


## Step 5: Monitor Progress
You can monitor the progress of the pipeline by checking the output directories specified in `gpa$output_locations`. The pipeline will create a structured output directory for each analysis, including directories for Level 1, Level 2, and Level 3 analyses, as well as job submission scripts and logs.

---

# Output directory structure
`fmri.pipeline` organizes outputs in a structured way similar to Brain Imaging Data Structure (BIDS) format. 
The output directory structure is as follows:
* name of the analysis
  * feat_l1: First-level analysis outputs
    * sub-{id}: Subject-specific directory
      * ses-{session}: Session-specific directory (if applicable)
        * {l1_model_name}: Directory for the specific Level 1 model
          * FEAT_LV1_run{run_number}.feat: FSL Feat folders for each run being executed by the scheduler
          * FEAT_LV1_run{run_number}.fsf: FSL Feat design file containing the model specification in FSL syntax
          * {model}_bdm_setup.RData: ??
          * FEAT_LV2_{l2_model_name}.gfeat/fsf: Combine all runs into one subject level analysis. Directory for the specific Level 2 model (if applicable)
          * timing_files: folder ??

  * feat_l3: Third-level analysis outputs
    * {l1_model_name}: Directory for the specific Level 1 model
      * L2m-{l2_model_name}_l2c-{l2_contrast}: Directory for the specific Level 2 model
      * L2m-{l2_model_name}_l2c-overall: generated if in L2 building step you say "yes" to do you want to get the overall response (averaging over all the covariates in the level 2 model).
        * L3m-{l3_model_name}: Directory for the specific Level 3 model. If you want to look at a specific regressor for a level 1 model and look at a specific contrast, you will look into these directories.

  * scheduler_scripts: Directory containing job submission scripts for the scheduler (e.g., SLURM, Torque)
    * batch_{batch_id}: Directory for a specific batch of jobs. Everytime `run_glm_pipeline()` is called, a new batch directory is created.
      * Rscripts for each step that fmri.pipeline runs
      * Rout files providing comments on each steps run by each Rsciript. To determine how far the pipeline progressed, check the latest .Rout file in the batch directory.
      * RData file containing the `gpa` object with updated information about the analysis.
      * {job_name}.sh: Job submission script for the specific job
      * {job_name}.out: Output file for the job run

  * setup_l{model_level}_models.txt: log files. For example, `setup_l3_models.txt` contains information about which runs for which subjects are excluded due to excessive head motion.

  * .sqlite: SQLite database for tracking analysis progress and results

`fmri.pipeline` also gives considerable flexibility in how you want to structure your outputs by changing the contents of gpa$output_locations object. The output location can be a dynamic expression (based on `glue` R package) that gets evaluated when the package is run. This is the default expresson for first-level FEAT directories: `"{gpa$output_directory}/feat_l1/sub-{id}/ses-{session}/{l1_model_name}"`. The contains inside the curly brackets are evaluated at runtime, so you can use any variable in the `gpa` object to specify the output location.


---
